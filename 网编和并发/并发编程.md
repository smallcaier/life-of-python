# 并发编程

## 1 操作系统简单介绍

​	1.1  多道技术

```python
操作系统实现的技术
	空间复用   多个程序同时在跑道内存中
	时间复用   遇到IO自动切换,把其他程序的IIO阻塞时间充分的利用了

分时技术
	伪并行: 看上去是同时执行  单纯的切换执行并不会提高效率,遇到IO切换才会提高效率
300MIPS      
```



## 2.进程

```
并发  伪并行,遇到IO自动切换, 两步:任务切换+保存状态

并行  多核cpu,多个程序同时运行
```

​	2.1 进程

```
行进当中的程序,系统的最小资源单位
```

​	2.2 进程三状态

```
就绪  执行  阻塞 --- 一直到程序结束
```

​	2.3 同步\异步  阻塞\非阻塞

```
任务的两种提交方式:同步\异步
同步:类似于串行,前面的任务执行完成之后下一个任务才能提交,才能执行
异步:不需要等待前一个任务的执行,可以直接提交任务并且程序不等待,大家并发执行
专业解释:
	所谓同步就是一个任务的完成需要依赖另外一个任务时，只有等待被依赖的任务完成后，依赖的任务才能算完成，这是一种可靠的任务序列。要么成功都成功，失败都失败，两个任务的状态可以保持一致。其实就是一个程序结束才执行另外一个程序，串行的，不一定两个程序就有依赖关系。

	所谓异步是不需要等待被依赖的任务完成，只是通知被依赖的任务要完成什么工作，依赖的任务也立即执行，只要自己完成了整个任务就算完成了。至于被依赖的任务最终是否真正完成，依赖它的任务无法确定，所以它是不可靠的任务序列。
	
任务的执行状态  阻塞\非阻塞
	
	
同步阻塞形式
　　　　效率最低。拿上面的例子来说，就是你专心排队，什么别的事都不做。

异步阻塞形式
　　　　如果在排队取餐的人采用的是异步的方式去等待消息被触发（通知），也就是领了一张小纸条，假如在这段时间里他不能做其它的事情，就在那坐着等着，不能玩游戏等，那么很显然，这个人被阻塞在了这个等待的操作上面；

　　　　异步操作是可以被阻塞住的，只不过它不是在处理消息时阻塞，而是在等待消息通知时被阻塞。

同步非阻塞形式
　　　　实际上是效率低下的。

　　　　想象一下你一边打着电话一边还需要抬头看到底队伍排到你了没有，如果把打电话和观察排队的位置看成是程序的两个操作的话，这个程序需要在这两种不同的行为之间来回的切换，效率可想而知是低下的。

异步非阻塞形式
　　　　效率更高，

　　　　因为打电话是你(等待者)的事情，而通知你则是柜台(消息触发机制)的事情，程序没有在两种不同的操作中来回切换。

　　　　比如说，这个人突然发觉自己烟瘾犯了，需要出去抽根烟，于是他告诉点餐员说，排到我这个号码的时候麻烦到外面通知我一下，那么他就没有被阻塞在这个等待的操作上面，自然这个就是异步+非阻塞的方式了。

　　很多人会把同步和阻塞混淆，是因为很多时候同步操作会以阻塞的形式表现出来，同样的，很多人也会把异步和非阻塞混淆，因为异步操作一般都不会在真正的IO操作处被阻塞。
```



## 3.进程的两种创建方式

```python
import os
from multiprocessing import Process
# 方式1
# def f1(n):
#     print(os.getpid())
#     print(os.getppid())
#     print(n)
# if __name__ == '__main__':  #进程的创建在windows下,类似于import
#     p = Process(target=f1,args=(2,),name='进程1号')
#     p.start()  #给操作系统发送创建进程的信号
#     # p.pid
#     print(p.name)
#     print('主进程执行结束')

#方式2
class MyProcess(Process):
    def __init__(self,n,m):
        self.n = n
        self.m = m
        super().__init__()
    def run(self):
        print('子进程')
        print(self.n,self.m)
        # self.xx()
    # def xx(self):
    #     print('xxx')
if __name__ == '__main__':
    p = MyProcess('taibai','dsb')
    p.start()
```



## 	4.进程之间是空间隔离的

​	5.join

```python
等待子进程结束再继续执行
示例:
def f1(n):
    print(n)
if __name__ == '__main__':  
    p = Process(target=f1,args=(2,),name='进程1号')
    p.start()  
    p.join()  #父进程在这个位置子进程执行结束
    print('主进程执行结束')
```



## 	6.for循环创建进程

```python
示例:
def f1(n):
    print(n)
    
if __name__ == '__main__':
    p_list = []
    for i in range(10):
        p = Process(target=f1,args=(i,))
        p.start()
        p_list.append(p)
    
    [pp.join() for pp in p_list]
        
    print('主进程执行结束')
```

## 	7.守护进程

```python
主进程代码运行结束,守护进程会随之结束
代码示例:
p = Process(target=f1,args=(i,))
p.daemon = True  #守护进程  在start之前写
p.start()

```

## 	8.锁\互斥锁\同步锁

```python
并发执行任务,多个任务同时操作共享数据的时候,会造成数据混乱的问题.
操作共享数据的代码部分要加锁处理
from multiprocessing import Process,Lock
def f1(n,loc):
    loc.acquire()
    time.sleep(1)
    print(n)
    loc.release()
    
    # with loc:
    #     代码
if __name__ == '__main__':
    loc = Lock()
    for i in range(10):
        p = Process(target=f1,args=(i,loc))
        p.daemon = True  #守护进程
        p.start()
    print('主进程执行结束')

```



## 	9.死锁现象和递归锁RLock

```python
线程和进程的死锁现象是一样的,加锁也是一样的写法.
线程:from threading import Thread,Lock
进程:from multiprocessing import Process,Lock

锁嵌套,引起的死锁现象,你中有我,我中有你,双方互相抢对方已经拿到的锁,导致双方互相等待
import time
from multiprocessing import Process,Lock,RLock
def f1(locA,locB):
    locA.acquire()
    print('任务1拿到了A锁')
    time.sleep(1)
    locB.acquire()
    print('任务1拿到了B锁')
    locB.release()
    locA.release()

if __name__ == '__main__':
    # locA = locB = Lock()
    locA = locB = RLock()  #递归锁解决死锁现象,内部维护了一个计数器,acquire一次加1,release一次减1,直到为0,大家才能继续抢这个锁,递归锁本身就是互斥锁
    p = Process(target=f1,args=(locA,locB))
    p.start()

#死锁现象2
import time
from multiprocessing import Process,Lock,RLock

def f1(locA,locB):

    locA.acquire()
    print('任务1拿到了A锁')
    time.sleep(1)
    locB.acquire()
    print('任务1拿到了B锁')
    locB.release()
    locA.release()

def f2(locA, locB):
    locB.acquire()
    print('任务2拿到了A锁')
    time.sleep(1)
    locA.acquire()
    print('任务2拿到了B锁')
    locA.release()
    locB.release()


if __name__ == '__main__':
    # locA =  Lock()
    # locB = Lock()
    locA = locB = RLock()  #递归锁解决死锁现象,内部维护了一个计数器,acquire一次加1,release一次减1,直到为0,大家才能继续抢这个锁,递归锁本身就是互斥锁

    p1 = Process(target=f1,args=(locA,locB))
    p2 = Process(target=f2,args=(locA,locB))
    p1.start()
    p2.start()

```



## 	10.GIL锁 全局解释器锁

```
加在Cpython解释器上的一把互斥锁,同一时间是能解释一个线程,导致多线程应用不了多核
```

## 	11.进程队列

```python
from multiprocessing import Process,Queue
#先进先出,数据取一次就没有了,实现机制:锁+管道. 保证数据安全可靠,进程安全的

q = Queue(3)
q.put(11)
q.put(22)
# print(q.qsize())  #当前队列中有多少条数据
q.put(33)

# print(q.empty())  #判断队列是否为空  他们两个都是不可信的
# print(q.full())  #判断队列是否满了
# q.put(44)  #队列满了,会阻塞程序
try:
    q.put_nowait(44)  # 报错:queue.Full  满了
except Exception:
    print('队列满了!')

print(q.get())
print(q.get())
print(q.get())
# print(q.get())
# print(q.get_nowait())  #报错:queue.Empty  空了

try:
    print(q.get_nowait())   # 报错:queue.Full  满了
except Exception:
    print('队列kongle!')

print('xing')
```

```python
进程队列实现进程通信
from multiprocessing import Process,Queue

def f1(q):
    print('子进程>>',q.get())

if __name__ == '__main__':
    q = Queue(3)
    p = Process(target=f1,args=(q,))
    p.start()
    q.put('你好')
```

数据共享 manager  管道Pipe  事件Event  信号量semaphore   自行去看看



## 线程

​	cpu最小执行单位,每个进程里面必然有一个线程.

### 	1.线程的两种创建方式

```python
from threading import Thread

#方式1
# def f1(n):
#     print(n)
# 
# if __name__ == '__main__':
#     t  =Thread(target=f1,args=(2,))
#     t.start()


#方式二
# class MyThread(Thread):
#
#     def __init__(self,n):
#         super().__init__()
#         self.n = n
#
#     def run(self):
#         print(self.n)
#         pass
# if __name__ == '__main__':
#     t = MyThread('xx')
#     t.start()
```

### 	2.守护线程+join

```
from threading import Thread

#方式1
def f1(n):
    print(n)

if __name__ == '__main__':
    t  =Thread(target=f1,args=(2,))
    t.daemon = True  #等待所有非守护子线程的结束才结束
    t.start()
    t.join()  #父线程等待子线程运行结束才继续执行
```



### 	3.线程是数据共享的

```python
from  threading import Thread
from multiprocessing import Process
import os
def work():
    global n  #修改全局变量的值
    n=0

if __name__ == '__main__':
    # n=100
    # p=Process(target=work)
    # p.start()
    # p.join()
    # print('主',n) #毫无疑问子进程p已经将自己的全局的n改成了0,但改的仅仅是它自己的,查看父进程的n仍然为100


    n=1
    t=Thread(target=work)
    t.start()
    t.join()   #必须加join，因为主线程和子线程不一定谁快，一般都是主线程快一些，所有我们要等子线程执行完毕才能看出效果
    print('主',n) #查看结果为0,因为同一进程内的线程之间共享进程内的数据
# 通过一个global就实现了全局变量的使用，不需要进程的IPC通信方法
```



### 	4.线程锁\互斥锁\同步锁   死锁现象  递归锁  GIL锁

​	

### 	5.线程队列

```python
import queue
#先进先出
q = queue.Queue(3)
# ....

#先进后出,后进先出
q = queue.LifoQueue(3)
# ....

#优先级队列
q = queue.PriorityQueue(3)
q.put((3,'bb'))  #数字表示优先级,数字越小,优先级越高,越优先被取出来
q.put((1,'aa'))  #数字表示优先级,数字越小,优先级越高,越优先被取出来

print(q.get())
print(q.get())
# 结果
# (1, 'aa')
# (3, 'bb')
```

​	

### 	6.线程池

```python
from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor
import time

def f1(n):
    print(n)
    time.sleep(1)
    return n+1

def call_back(m):
    print(m.result())

if __name__ == '__main__':
    # pool = ThreadPoolExecutor(max_workers=3)
    pool = ProcessPoolExecutor(max_workers=3)
    res = pool.submit(f1,11,) #异步提交任务
    print('>>>',res.result())  #阻塞程序
    # res = pool.submit(f1, 22, )
    # pool.shutdown()  #关闭或者锁定线程池,

    pool.map(f1,range(10))  #异步提交的任务

    res = pool.submit(f1, 11,).add_done_callback(call_back)  #回调函数,将前面提交的f1任务的返回结果对象,作为参数给了call_back函数,需要result来调结果

```



## 协程

​	微线程,提高单线程的执行效率.

​	单线程并发执行多任务,并发效果是通过代码实现的任务切换+保存状态

​	

​	生成器

​    greenlet模块,switch方法来切换任务,不能自动识别IO来进行自动切换

​	gevent模块  内部通过greenlet来实现的

```python
from gevent import monkey;monkey.patch_all()  #识别所有IO操作的补丁,必须写在最上面
import gevent
import time

def f1(n):
    time.sleep(1)
    # gevent.sleep(1)
    print('任务1',n)

def f2(n):
    time.sleep(1)
    print('任务2',n)

# f1()
# f2()

g1 = gevent.spawn(f1,11)
g2 = gevent.spawn(f2,22)
# g1.join()
gevent.joinall([g1,g2])

print('主任务')


```



参考博客:<https://www.cnblogs.com/clschao/articles/10065920.html>





























































